[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Analyzing PCB Content In Hudson River Fish\n\n\n\nR\n\n\nMEDS\n\n\n\nRunning statistical analyis in order to understand the concentration of PCB contaminants within the fish living in the Hudson River from 2001 to 2011\n\n\n\nBen Versteeg\n\n\nDec 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTexas Power Crisis\n\n\n\nR\n\n\nMEDS\n\n\n\nEstimating the number of homes in Houston that lost power as a result of the first two storms and investigating if socioeconomic factors are predictors of communities…\n\n\n\nBen Versteeg\n\n\nDec 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAir Quality Data & False Color Image of the Thomas Fire\n\n\n\nPython\n\n\nMEDS\n\n\n\n\n\n\n\nBen Versteeg\n\n\nDec 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy First Blog Post\n\n\n\nQuarto\n\n\nR\n\n\nMEDS\n\n\n\nthis is my short description\n\n\n\nBen Versteeg\n\n\nNov 6, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/Texas_Power_Crisis/Texas_Power_Crisis.html",
    "href": "blog/Texas_Power_Crisis/Texas_Power_Crisis.html",
    "title": "Texas Power Crisis",
    "section": "",
    "text": "Link to repository: https://github.com/BenVerst/Texas_Power_Crisis"
  },
  {
    "objectID": "blog/Texas_Power_Crisis/Texas_Power_Crisis.html#overview",
    "href": "blog/Texas_Power_Crisis/Texas_Power_Crisis.html#overview",
    "title": "Texas Power Crisis",
    "section": "Overview",
    "text": "Overview\n“In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1 For more background, check out these engineering and political perspectives.\nTopics this notebook will cover:\n- estimating the number of homes in Houston that lost power as a result of the first two storms\n- investigating if socioeconomic factors are predictors of communities recovery from a power outage\nThe analysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, we will use the VNP46A1 to detect differences in night lights before and after the storm to identify areas that lost electric power.\nTo determine the number of homes that lost power, we will link (spatially join) these areas with OpenStreetMap data on buildings and roads.\nTo investigate potential socioeconomic factors that influenced recovery, the analysis will be linked with data from the US Census Bureau.\n \n\nHighlights\n\nload vector/raster data\nsimple raster operations\nsimple vector operations\nspatial joins"
  },
  {
    "objectID": "blog/Texas_Power_Crisis/Texas_Power_Crisis.html#data",
    "href": "blog/Texas_Power_Crisis/Texas_Power_Crisis.html#data",
    "title": "Texas Power Crisis",
    "section": "Data",
    "text": "Data\n\nNight lights\nUse NASA’s Worldview to explore the data around the day of the storm. There are several days with too much cloud cover to be useful, but 2021-02-07 and 2021-02-16 provide two clear, contrasting images to visualize the extent of the power outage in Texas.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06. We therefore need to download two tiles per date.\nAccessing, downloading, and preparing remote sensing data is a skill in it’s own right! To prevent this notebook from being a large data wrangling challenge, the following files are prepped and stored in the VNP46A1 folder.\n\n\nVNP46A1.A2021038.h08v05.001.2021039064328.h5.tif: tile h08v05, collected on 2021-02-07\n\nVNP46A1.A2021038.h08v06.001.2021039064329.h5.tif: tile h08v06, collected on 2021-02-07\n\nVNP46A1.A2021047.h08v05.001.2021048091106.h5.tif: tile h08v05, collected on 2021-02-16\n\nVNP46A1.A2021047.h08v06.001.2021048091105.h5.tif: tile h08v06, collected on 2021-02-16\n\n\n\n\nRoads\nTypically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, we will ignore areas near highways.\nOpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. Ingesting this data into a database where it can be subsetted and processed is a large undertaking. Fortunately, third party companies redistribute OSM data. We used Geofabrik’s download sites to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area. \n\ngis_osm_roads_free_1.gpkg\n\n\n\n\nHouses\nWe can also obtain building data from OpenStreetMap. We again downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\n\n\ngis_osm_buildings_a_free_1.gpkg\n\n\n\n\nSocioeconomic\nWe cannot readily get socioeconomic information for every home, so instead we obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file.\n\nWe can use st_layers() to explore the contents of the geodatabase. Each layer contains a subset of the fields documents in the ACS metadata.\n\nThe geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes. We’ll have to combine the geometry with the attributes to get a feature layer that sf can use."
  },
  {
    "objectID": "blog/Texas_Power_Crisis/Texas_Power_Crisis.html#find-locations-of-blackouts",
    "href": "blog/Texas_Power_Crisis/Texas_Power_Crisis.html#find-locations-of-blackouts",
    "title": "Texas Power Crisis",
    "section": "Find Locations of Blackouts",
    "text": "Find Locations of Blackouts\nFor improved computational efficiency and easier interoperability with sf, it is recommend to use the stars package for raster handling.\n\n\n\nCombining the Data\nSteps:\n\nread in night lights tiles\n\ncombine tiles into a single stars object for each date (2021-02-07 and 2021-02-16)\n\n\n\nLoading in necessary packages\n\n\nCode\nlibrary(stars)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(raster)\nlibrary(terra)\nlibrary(ggplot2)\n\n\n\n\n\nReading in the tile data\n\n\nCode\nnight_lights07_1 &lt;- read_stars(\"~/MEDS/MEDS_223/Projects/Texas_Power_Crisis/data/VNP46A1/VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.tif\")\n\nnight_lights07_2 &lt;- read_stars(\"~/MEDS/MEDS_223/Projects/Texas_Power_Crisis/data/VNP46A1/VNP46A1/VNP46A1.A2021038.h08v06.001.2021039064329.tif\")\n\nnight_lights16_1 &lt;- read_stars(\"~/MEDS/MEDS_223/Projects/Texas_Power_Crisis/data/VNP46A1/VNP46A1/VNP46A1.A2021047.h08v05.001.2021048091106.tif\")\n\nnight_lights16_2 &lt;- read_stars(\"~/MEDS/MEDS_223/Projects/Texas_Power_Crisis/data/VNP46A1/VNP46A1/VNP46A1.A2021047.h08v06.001.2021048091105.tif\")\n\n\n\n\n\nCombining data into single stars objects for each day\n\n\nCode\nnight_lights07 &lt;- st_mosaic(night_lights07_1, night_lights07_2)\nnight_lights16 &lt;- st_mosaic(night_lights16_1, night_lights16_2)\n\n\n\n\n\n\nCreating a Blackout Mask\nThe provided code details the process of generating a blackout mask specific to the Houston area, which was utilized for identifying affected homes. The underlying assumption was that any site registering a decrease exceeding 200 nW cm-2sr-1 was indicative of a blackout.\nSteps:\n\nfind the change in night lights intensity (presumably) caused by the storm\nreclassify the difference raster, assuming that any location that experienced a drop of more than 200 nW cm-2sr-1 experienced a blackout\nassign NA to all locations that experienced a drop of less than 200 nW cm-2sr-1\nuse st_as_sf() to vectorize the blackout mask\nfix any invalid geometries using st_make_valid\ndefine the Houston metropolitan area with the following coordinates\n\n(-96.5, 29), (-96.5, 30.5), (-94.5, 30.5), (-94.5, 29)\n\nturn these coordinates into a polygon using st_polygon\nconvert the polygon into a simple feature collection using st_sfc() and assign a CRS\ncrop (spatially subset) the blackout mask to the Houston area \nre-project the cropped blackout dataset to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area)\n\n\n\n\nCode\n# light difference between the two dates \nlight_dif &lt;- night_lights07 - night_lights16\n\n# assigned NA to all locations that experienced a drop of less than 200 nW cm-2sr-1\nblackout_mask &lt;- cut(light_dif, c(200, Inf), labels = \"NA\")\n\n# vectorize blackout mask and fix invalid geometries\nblackout_vector &lt;- st_as_sf(blackout_mask) %&gt;%  \n  st_make_valid()\n\n# defining Houston area with coords\nhoust_coords &lt;- matrix(c(-96.5, 29, -96.5, 30.5, -94.5, 30.5, -94.5, 29, -96.5, 29), ncol = 2, byrow = TRUE)\n  \n# creating a polygon of Houston's coordinates and crs\nhoust_poly &lt;- st_polygon(list(houst_coords)) %&gt;% st_sfc(crs = 4326)\n\n# cropping the blackout mask with Houston polygon\nhoust_crop &lt;- blackout_vector[houst_poly, ,]\n\n# reproject cropped dataset, add 3083 crs and convert to sf\nhoust_3083 &lt;- st_transform(houst_crop, crs = 3083) %&gt;% st_as_sf(houst_crop)\n\n\n\n\n\nExcluding Highways From Blackout Mask\nTo mitigate the impact of highway lighting, we employed a strategy that entailed creating a 200-meter buffer, ensuring that only regions situated more than 200 meters away from a highway were retained within our blackout mask. By taking advantage of st_read’s capability to subset using a SQL query, we can avoid reading unnecessary data from the roads geopackage, which includes information on roads other than highways.\nSteps:\n\ndefine SQL query\n\nload just highway data from geopackage using st_read\n\nreproject data to EPSG:3083\n\nidentify areas within 200m of all highways using st_buffer\n\nfind areas that experienced blackouts that are further than 200m from a highway\n\nquery &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\nhighways &lt;- st_read(\"data/gis_osm_roads_free_1.gpkg\", query = query)\n\n\nCode\n# read in highway data using SQL query and st_read()\nquery &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\nhighways &lt;- st_read(\"~/MEDS/MEDS_223/Projects/assignment-3-BenVerst/data/gis_osm_roads_free_1.gpkg/gis_osm_roads_free_1.gpkg\", query = query)\n\n\nReading query `SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway''\nfrom data source `C:\\Users\\17143\\Documents\\MEDS\\MEDS_223\\Projects\\assignment-3-BenVerst\\data\\gis_osm_roads_free_1.gpkg\\gis_osm_roads_free_1.gpkg' \n  using driver `GPKG'\nSimple feature collection with 6085 features and 10 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -96.50429 ymin: 29.00174 xmax: -94.39619 ymax: 30.50886\nGeodetic CRS:  WGS 84\n\n\nCode\n# select the geometry highway data\nhighway_geom &lt;- highways$geom\n\n# use st_transform to make the highway geometries to 3083 crs\nhighway_geom &lt;- st_transform(highway_geom, crs = 3083)\n\n# create a buffer zone of 200 meters\nhighway_buffer &lt;- st_buffer(x = highway_geom, dist = 200)\n\n# combine the geometries and create a mask that excludes highway data\nhighway_buffer &lt;- st_union(highway_buffer, by_feature = FALSE)\nhoust_high_mask &lt;- houst_3083[highway_buffer, , op = st_disjoint]"
  },
  {
    "objectID": "blog/Texas_Power_Crisis/Texas_Power_Crisis.html#find-homes-impacted-by-blackouts",
    "href": "blog/Texas_Power_Crisis/Texas_Power_Crisis.html#find-homes-impacted-by-blackouts",
    "title": "Texas Power Crisis",
    "section": "Find Homes Impacted by Blackouts",
    "text": "Find Homes Impacted by Blackouts\nThe code provided below explains the process of utilizing the recently generated blackout mask in conjunction with highway data to identify homes most likely affected by the power outage and ascertain the number of impacted residences.\nSteps:\n\nload buildings dataset using st_read and the following SQL query to select only residential buildings\n\nreproject data to EPSG:3083\nfilter to homes within blackout areas\ncount number of impacted homes\n\n\nSELECT *  FROM gis_osm_buildings_a_free_1\nWHERE (type IS NULL AND name IS NULL)\nOR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\n\n\n\nCode\n# define query\nquery_houses &lt;- \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\n# read in highway data with SQL\nhouses &lt;- st_read(\"~/MEDS/MEDS_223/Projects/assignment-3-BenVerst/data/gis_osm_buildings_a_free_1.gpkg/gis_osm_buildings_a_free_1.gpkg\", query = query_houses)\n\n\nReading query `SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')'\nfrom data source `C:\\Users\\17143\\Documents\\MEDS\\MEDS_223\\Projects\\assignment-3-BenVerst\\data\\gis_osm_buildings_a_free_1.gpkg\\gis_osm_buildings_a_free_1.gpkg' \n  using driver `GPKG'\nSimple feature collection with 475941 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -96.50055 ymin: 29.00344 xmax: -94.53285 ymax: 30.50393\nGeodetic CRS:  WGS 84\n\n\nCode\nhouses &lt;- st_transform(houses, \"EPSG:3083\")\n\n# filtering \noutage_houses &lt;- houses[houst_high_mask ,drop = FALSE]"
  },
  {
    "objectID": "blog/Texas_Power_Crisis/Texas_Power_Crisis.html#investigate-socioeconomic-factors",
    "href": "blog/Texas_Power_Crisis/Texas_Power_Crisis.html#investigate-socioeconomic-factors",
    "title": "Texas Power Crisis",
    "section": "Investigate Socioeconomic Factors",
    "text": "Investigate Socioeconomic Factors\nWith data on the affected houses at hand, we can now align this information with socioeconomic census tract data. This allows us to identify and determine the census tracts that were impacted by the power outage.\n\nLoading in ACS Data\nSteps:\n\nuse st_read() to load the geodatabase layers\n\ngeometries are stored in the ACS_2019_5YR_TRACT_48_TEXAS layer\n\nincome data is stored in the X19_INCOME layer\n\nselect the median income field B19013e1\n\nreproject data to EPSG:3083\n\n\n\n\nCode\n#reading in geometry data\ncensus_geom &lt;- st_read(\"~/MEDS/MEDS_223/Projects/assignment-3-BenVerst/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer = \"ACS_2019_5YR_TRACT_48_TEXAS\")\n\n\nReading layer `ACS_2019_5YR_TRACT_48_TEXAS' from data source \n  `C:\\Users\\17143\\Documents\\MEDS\\MEDS_223\\Projects\\assignment-3-BenVerst\\data\\ACS_2019_5YR_TRACT_48_TEXAS.gdb\\ACS_2019_5YR_TRACT_48_TEXAS.gdb' \n  using driver `OpenFileGDB'\nSimple feature collection with 5265 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -106.6456 ymin: 25.83716 xmax: -93.50804 ymax: 36.5007\nGeodetic CRS:  NAD83\n\n\nCode\ncensus_geom &lt;- st_transform(census_geom, crs = 3083)\n\n#reading in income data\nmed_income &lt;- st_read(\"~/MEDS/MEDS_223/Projects/assignment-3-BenVerst/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer = \"X19_INCOME\")\n\n\nReading layer `X19_INCOME' from data source \n  `C:\\Users\\17143\\Documents\\MEDS\\MEDS_223\\Projects\\assignment-3-BenVerst\\data\\ACS_2019_5YR_TRACT_48_TEXAS.gdb\\ACS_2019_5YR_TRACT_48_TEXAS.gdb' \n  using driver `OpenFileGDB'\n\n\nCode\n#selecting for my coloums\nsel_med_income &lt;- med_income |&gt; \n  dplyr::select(\"GEOID\", \"B19013e1\") |&gt; \n  rename(GEOID_Data = GEOID, median_income = B19013e1)\n\n\n\n\n\nDetermining Which Census Tracts Experienced Blackouts\nSteps:\n\njoin the income data to the census tract geometries\n\nspatially join census tract data with buildings determined to be impacted by blackouts\n\nfind which census tracts had blackouts\n\n\n\n\nCode\n# check the class\nclass(census_geom)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nCode\nclass(sel_med_income)\n\n\n[1] \"data.frame\"\n\n\nCode\n# change to a data_frame\nsel_med_income_df &lt;- tibble(sel_med_income)\n\n# join census and median income data\ncensus_data &lt;- left_join(census_geom, \n                         sel_med_income, \n                         by = \"GEOID_Data\")\n\n# change to the 4326 crs\ncensus_data &lt;- st_transform(census_data, crs = 4326)\noutage_houses &lt;- st_transform(outage_houses, crs = 4326)\n\n# check the class\nclass(census_data)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nCode\nclass(outage_houses)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nCode\n# filter census data using the outage houses. also add a column which tracts were a blackout\ncensus_outage &lt;- sf::st_filter(census_data, outage_houses) |&gt; \n  mutate(blackout = 'yes')"
  },
  {
    "objectID": "blog/Texas_Power_Crisis/Texas_Power_Crisis.html#comparing-incomes-of-impacted-tracts-to-unimpacted-tracts",
    "href": "blog/Texas_Power_Crisis/Texas_Power_Crisis.html#comparing-incomes-of-impacted-tracts-to-unimpacted-tracts",
    "title": "Texas Power Crisis",
    "section": "Comparing Incomes of Impacted Tracts to Unimpacted Tracts",
    "text": "Comparing Incomes of Impacted Tracts to Unimpacted Tracts\nThe provided code streamlines the data wrangling processes necessary for visualizations. It encompasses the creation of maps and plots to compare census tracts that underwent a blackout with those that did not.\nSteps:\n\ncreate a map of median income by census tract, designating which tracts had blackout\nplot the distribution of income in impacted and unimpacted tracts\n\n\n\nCode\n# crop census data with Houston border\nhoust_census_data &lt;- census_data[houst_poly, ,] \n\n# transform census data to 3083 crs\nhoust_census_data &lt;- st_transform(houst_census_data, crs = 3083)\n\n# select columns for houston census\nhoust_census_data &lt;- houst_census_data |&gt; \n  dplyr::select(\"NAMELSAD\", \"Shape\", \"median_income\", \"GEOID_Data\")\n\n# select columns for outage data by census track\ncensus_outage &lt;- census_outage |&gt; \n  dplyr::select(\"blackout\", \"GEOID_Data\")\ncensus_outage_map &lt;- census_outage |&gt; \n  dplyr::select(\"blackout\")\n\n# convert census outage data to dataframe\ncensus_outage_df &lt;- as.data.frame(census_outage)\n\n# join census outage data and census data for Houston\ncensus_map_data &lt;- left_join(houst_census_data, \n                             census_outage_df, \n                             by = \"GEOID_Data\")\n\ncensus_map_data &lt;- census_map_data |&gt; \n  dplyr::select('median_income', 'blackout')\n\n\n\nMapping median income by census track and identifying outages\n\n\nCode\ntm_shape(census_map_data) +\n\n  # Add polygons representing median income with a heat color palette\n  tm_polygons(\"median_income\",\n              palette = 'heat',\n              textNA = \"Missing Income Data\", # Label for areas with missing income data\n              title = \"Median Income\") + # Title\n\n  # Add a new layer using census_outage_map\n  tm_shape(census_outage_map) +\n\n    # Add dots to represent outages\n    tm_dots(shape = 16,        # Shape code for filled circles\n            size = .2,         # Size of the dots\n            col = \"black\",     # Set the color of dots to black\n            title = 'blackout') +  # Legend title for the dots\n\n  # Set the layout options for the entire map\n  tm_layout(main.title = \"Houston Income Census Tracts that Had A Blackout\", # Main title of the map\n            legend.outside = TRUE,  # Display the legend outside the map\n            main.title.size = 1)    # Set the size of the main title\n\n\n\n\n\n\n\nPlotting census data that experienced a blackout\n\n\nCode\n# convert census map data to dataframe\ncensus_plot_data &lt;- data_frame(census_map_data)\n\n# add homes that didn't have a blackout\ncensus_plot_data &lt;- census_plot_data |&gt; \n  mutate(blackout = replace(blackout, is.na(blackout), \"no\"))\n\n# dataframe for homes with a blackout\ncensus_plot_data_blackout &lt;- census_plot_data |&gt; \n  dplyr::select(\"median_income\", \"blackout\") |&gt; \n  filter(blackout == \"yes\")\n\n\n# dataframe for homes with no blackout\ncensus_plot_data_no_blackout &lt;- census_plot_data |&gt; \n  dplyr::select(\"median_income\", \"blackout\") |&gt; \n  filter(blackout == \"no\")\n\n# Create a histogram using ggplot with census_plot_data_blackout\nggplot(census_plot_data_blackout, aes(x = median_income)) +\n  # Add histogram bars with black outline and dark green fill\n  geom_histogram(color = \"black\", fill = \"darkgreen\") +\n  labs(\n    title = \"Median Income for Homes that Had a Blackout\", # Title\n    x = \"Median Income\",                                   # X-axis label\n    y = \"Count\"                                            # Y-axis label\n  ) +\n  # Set the theme to minimal\n  theme_minimal()\n\n\n\n\n\n\n\nPlotting census data that didn’t experience a blackout\n\n\nCode\n# Create a histogram using ggplot with census_plot_data_no_blackout\nggplot(census_plot_data_no_blackout, aes(x = median_income)) +\n  # Add histogram bars with black outline and light green fill\n  geom_histogram(color = \"black\", fill = \"lightgreen\") +\n  labs(\n    title = \"Median Income for Homes that Didn't Have a Blackout\", # Plot title\n    x = \"Median Income\",                                           # X-axis label\n    y = \"Count\"                                                    # Y-axis label\n  ) +\n  # Set the theme to minimal\n  theme_minimal()\n\n\n\n\n\n\n\nCreating a scatter plot comparing median income for homes that had a blackout or not\n\n\nCode\nggplot(census_plot_data, aes(x = blackout, y = median_income)) +\n  # Add jittered points for better visualization\n  geom_jitter(width = 0.1,          # Width of jitter\n              height = 0,           # Height of jitter\n              color = \"purple\",     # Point color\n              alpha = 0.8) +        # Transparency of points\n  labs(\n    title = \"Comparison Between the Median Income for Homes that Had a Blackout or Not\",  # Plot title\n    x = \"Experienced Blackout\",  # X-axis label\n    y = \"Median Income\"          # Y-axis label\n  ) +\n  # Set the theme to minimal\n  theme_minimal()\n\n\n\n\n\nCode\n# Display summary statistics for homes that had a blackout\nsummary(census_plot_data_blackout)\n\n\n median_income      blackout        \n Min.   : 13886   Length:710        \n 1st Qu.: 43735   Class :character  \n Median : 60634   Mode  :character  \n Mean   : 71435                     \n 3rd Qu.: 89864                     \n Max.   :250001                     \n NA's   :3                          \n\n\nCode\n# Display summary statistics for homes that didn't have a blackout\nsummary(census_plot_data_no_blackout)\n\n\n median_income      blackout        \n Min.   : 24024   Length:402        \n 1st Qu.: 43383   Class :character  \n Median : 57049   Mode  :character  \n Mean   : 67494                     \n 3rd Qu.: 80796                     \n Max.   :250001                     \n NA's   :7                          \n\n\n\n\nHaving identified the average median income for homes in the Houston metropolitan area affected by the blackout during Texas’s 2021 energy crisis, this study revealed that the average median income for homes experiencing a blackout was $71,435. Notably, this figure exceeded the average median income for homes that did not experience a blackout, which stood at $64,494.\nHowever, it’s essential to note that this study did not consider the distribution of homes across lower and higher median income census tracts. Consequently, all census tracts were treated equally when calculating the average median income. Further investigations could explore grouping census tracts by income levels to assess the percentage of impacted versus non-impacted homes in each category. This approach aims to determine if lower median income levels were disproportionately affected compared to higher median income levels. Additionally, the study excluded homes within 200 meters of highways, potentially introducing a bias against homes with lower median incomes. Moreover, the analysis focused solely on median income within census tracts, neglecting other socioeconomic factors and medical vulnerability considerations.\nIt is crucial to emphasize that the primary objective of this investigation was to gain familiarity with spatial data. The results and findings presented here are preliminary and should not be cited without additional research. Overall, the aim was to provide insight into the use of different packages and functions for working with spatial data."
  },
  {
    "objectID": "blog/Texas_Power_Crisis/Texas_Power_Crisis.html#footnotes",
    "href": "blog/Texas_Power_Crisis/Texas_Power_Crisis.html#footnotes",
    "title": "Texas Power Crisis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWikipedia. 2021. “2021 Texas power crisis.” Last modified October 2, 2021. https://en.wikipedia.org/wiki/2021_Texas_power_crisis.↩︎"
  },
  {
    "objectID": "blog/2023-11-06-first_post/index.html",
    "href": "blog/2023-11-06-first_post/index.html",
    "title": "My First Blog Post",
    "section": "",
    "text": "CitationBibTeX citation:@online{versteeg2023,\n  author = {Versteeg, Ben},\n  title = {My {First} {Blog} {Post}},\n  date = {2023-11-06},\n  url = {https://github.com/BenVerst/BenVerst.github.io/tree/master/blog/2023-11-06-first_post},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVersteeg, Ben. 2023. “My First Blog Post.” November 6,\n2023. https://github.com/BenVerst/BenVerst.github.io/tree/master/blog/2023-11-06-first_post."
  },
  {
    "objectID": "blog/hudson_river_pcb_analysis/hudson_river_analysis.html",
    "href": "blog/hudson_river_pcb_analysis/hudson_river_analysis.html",
    "title": "Analyzing PCB Content In Hudson River Fish",
    "section": "",
    "text": "Hudson River"
  },
  {
    "objectID": "blog/hudson_river_pcb_analysis/hudson_river_analysis.html#main-questions",
    "href": "blog/hudson_river_pcb_analysis/hudson_river_analysis.html#main-questions",
    "title": "Analyzing PCB Content In Hudson River Fish",
    "section": "Main Questions",
    "text": "Main Questions\n\nDoes the amount of PCB found in the fish decrease over time?\nIs there a correlation between fish weight and PCB concentration?\nDoes the season have any effect on the PCB concentration in fish?"
  },
  {
    "objectID": "blog/hudson_river_pcb_analysis/hudson_river_analysis.html#introduction",
    "href": "blog/hudson_river_pcb_analysis/hudson_river_analysis.html#introduction",
    "title": "Analyzing PCB Content In Hudson River Fish",
    "section": "Introduction",
    "text": "Introduction\nThe Hudson River, a vital waterway in the United States, faced a formidable environmental challenge stemming from decades of polychlorinated biphenyls (PCBs) contamination up until the late 1970s. The Environmental Protection Agency (EPA) banned PCB production in 1977, however, it was estimated that 1.3 million pounds of PCBs had been discharged into the Hudson River prior to the ban. The widespread PCB contamination in the river’s sediments, raised concerns about its impact on aquatic ecosystems and human health, particularly through the consumption of contaminated fish. In response, the U.S. The Environmental Protection Agency (EPA) designated the Hudson River as a Superfund site, initiating an extensive cleanup effort. The Hudson River PCBs Superfund Cleanup involved dredging PCB-contaminated sediments from certain areas in 2009 and 2011 to 2015, aiming to reduce the overall PCB levels. This study will focus on the concentration of PCB contaminants within the fish living in the Hudson River from 2001 to 2011."
  },
  {
    "objectID": "blog/hudson_river_pcb_analysis/hudson_river_analysis.html#background",
    "href": "blog/hudson_river_pcb_analysis/hudson_river_analysis.html#background",
    "title": "Analyzing PCB Content In Hudson River Fish",
    "section": "Background",
    "text": "Background\nPCBs are a group of synthetic organic chemicals that were once widely used in various industrial applications. They were valued for their stability, electrical insulating properties, and resistance to heat and fire. However, PCBs are considered toxic, and exposure to high levels has been associated with a range of health issues in humans, including damage to the immune, reproductive, and nervous systems. They're also highly resistant to breaking down in the environment, leading to their persistence in soil, water, and air. PCBs can accumulate in the fatty tissues of animals and humans. This bioaccumulation occurs as PCBs move up the food chain, leading to higher concentrations in predators."
  },
  {
    "objectID": "blog/hudson_river_pcb_analysis/hudson_river_analysis.html#datamethods",
    "href": "blog/hudson_river_pcb_analysis/hudson_river_analysis.html#datamethods",
    "title": "Analyzing PCB Content In Hudson River Fish",
    "section": "Data/Methods",
    "text": "Data/Methods\nThe data that will be used in this analysis is a dataset collected from 2001-2011 on PCB levels in four different species of fish in the Hudson River by the New York Department of Environmental Conservation (NYSDEC). Data can be found here. The dataset contains information on the size, weight, sex, PCB content, and when and where the fish was caught. In order to collect this data, every year, the NYSDEC scientists collect approximately 2,000 fish from more than 50 different locations in New York water bodies. The caught fish are then sent to a lab where they are processed for PCB content. The plan in order to successfully answer the questions above was to run a summary statistic to see if there were any obvious trends in the data, a linear regression to see whether there is a correlation between the weight of the fish and PCB content, and a Seasonal and Trend decomposition using Loess (STL Decomposition) to see if there were any changes in PCB content based on the season.\n\n\nCode\n# importing libraries\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(tufte)\nlibrary(zoo)\nlibrary(feasts)\nlibrary(tsibble)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# read in data\npcb &lt;- read.csv(\"C:/Users/17143/Documents/MEDS/Quarto Websites/BenVerst.github.io/blog/hudson_river_pcb_analysis/data/PCBs in Hudson River Fish - Data.csv\")\n\n# update the dates into date format\npcb$SDATE &lt;- as.Date(as.character(pcb$SDATE), format = \"%Y%m%d\")\n\npcb$SDATE &lt;- format(pcb$SDATE, \"%Y-%m-%d\")"
  },
  {
    "objectID": "blog/hudson_river_pcb_analysis/hudson_river_analysis.html#analysisresults",
    "href": "blog/hudson_river_pcb_analysis/hudson_river_analysis.html#analysisresults",
    "title": "Analyzing PCB Content In Hudson River Fish",
    "section": "Analysis/Results",
    "text": "Analysis/Results\n\nData Exploration and Summary Statistic\n\n\nCode\n# plot data\nggplot(data = pcb) +\n  geom_point(aes(x = Year, y = Total.PCB.ppm.)) +\n  ggtitle(\"PCB Levels in Fish (2001-2011)\") +\n    labs(y = \"Total PCB (ppm)\") +\n  scale_x_continuous(breaks = pcb$Year, labels = pcb$Year)\n\n\n\n\n\n\n\nCode\n# taking the mean values\npcb_mean &lt;- pcb %&gt;%\n  group_by(Year) %&gt;%\n  summarize(mean_pcb = mean(Total.PCB.ppm., na.rm = TRUE))\n\n# plot mean values\nggplot(data = pcb_mean) +\n  geom_line(aes(x = Year, y = mean_pcb)) +\n  geom_smooth(aes(x = Year, y = mean_pcb), method = \"loess\", se = FALSE) +\n  ggtitle(\"Mean PCB Levels in Fish (2001-2011)\") +\n  labs(y = \"Mean PCB (ppm)\") +\n  scale_x_continuous(breaks = pcb_mean$Year, labels = pcb_mean$Year)\n\n\n\n\n\n\nThe figures above show the levels of PCB in the fish from 2001 and 2011. Based on these graphs, most PCB levels stay relatively low and constant except for a few potential outliers. In order to see a better representation of the trend, the outliers were removed.\n\n\n\nCode\n# removing outliers\npcb_edit &lt;- subset(pcb, Total.PCB.ppm. != 115.960)\npcb_edit &lt;- subset(pcb_edit, Total.PCB.ppm. != 86.850)\npcb_edit &lt;- subset(pcb_edit, Total.PCB.ppm. != 52.980)\npcb_edit &lt;- subset(pcb_edit, Total.PCB.ppm. != 36.610)\n\n# storing into a new data set\npcb_mean_edit &lt;- pcb_edit %&gt;%\n  group_by(Year) %&gt;%\n  summarize(mean_pcb_edit = mean(Total.PCB.ppm., na.rm = TRUE))\n\n# mean plot with no outliers and geom_smooth\nggplot(data = pcb_mean_edit) +\n  geom_line(aes(x = Year, y = mean_pcb_edit)) +\n  geom_smooth(aes(x = Year, y = mean_pcb_edit), method = \"loess\", se = FALSE) +\n  ggtitle(\"Mean PCB Levels in Fish Without Outliers (2001-2011)\") +\n  labs(y = \"Mean PCB (ppm)\") +\n  scale_x_continuous(breaks = pcb_mean_edit$Year, labels = pcb_mean_edit$Year)\n\n\n\n\n\n\n\nCode\n# summary statistics for each year\nsummary_by_year &lt;- pcb_edit %&gt;%\n  group_by(Year) %&gt;%\n  summarise(\n    Mean = mean(Total.PCB.ppm., na.rm = TRUE),\n    Median = median(Total.PCB.ppm., na.rm = TRUE),\n    Std_Dev = sd(Total.PCB.ppm., na.rm = TRUE)\n  )\n\n# Generate the table using kable and kableExtra\nsummary_by_year %&gt;%\n  kable(\"html\") %&gt;%\n  kable_classic(full_width = FALSE, html_font = \"Cambria\") %&gt;%\n  row_spec(c(1, 3, 5, 7, 9, 11), background = \"#E6F7FF\") %&gt;%\n  row_spec(c(0, 2, 4, 6, 8, 10), background = \"white\")\n\n\n\n\n\nYear\nMean\nMedian\nStd_Dev\n\n\n\n\n2001\n1.3274827\n0.8700\n1.2284199\n\n\n2002\n1.1199399\n0.7940\n1.0985864\n\n\n2003\n1.0783750\n0.7850\n1.0870688\n\n\n2004\n1.2557193\n0.5510\n2.0577301\n\n\n2005\n1.1284734\n0.5510\n1.7322199\n\n\n2006\n0.9462967\n0.4710\n1.2172877\n\n\n2007\n1.1420309\n0.7550\n1.0483810\n\n\n2008\n1.1204653\n0.6575\n1.2820803\n\n\n2009\n1.2407791\n0.5045\n1.5880153\n\n\n2010\n0.4479798\n0.2395\n0.6099138\n\n\n2011\n1.0778878\n0.4585\n1.5273802\n\n\n\n\n\n\n\n\nThe figure above displays the mean trend line of the data without outliers and the table shows a summary statistic of the data. Based on the data, it becomes apparent that there is a noticeable decline in PCB content in fish over the years with a few spikes every so often. However, What was very interesting was an extreme decrease in PCB content in the year 2010. While this may be confusing, recalling back to the background, the EPA began dredging the Hudson River in 2009, skipped 2010, and resumed in 2011. A possible explanation for this is when dredging began in 2009, the PCB in the sediment was stirred up causing frequent exposure of PCB to the fish. This also explains the spike in 2009. When dredging halted in 2010, the PCB settled and lowered exposure and in 2011, dredging resumed and stirred back up the sediment. \n\n\n\nLinear Regression\n\n\nCode\n# create a linear regression\npcb_lm &lt;- lm(Weight.g. ~ Total.PCB.ppm., data = pcb) %&gt;% \n  summary()\n\n# linear regression model without outliers\npcb_line &lt;- ggplot(data = pcb_edit, aes(x = Weight.g., y = Total.PCB.ppm.)) +\n  geom_point(color = \"darkgreen\") +\n  geom_smooth(method = lm, \n              se = TRUE,\n              alpha = 0.2,\n              color = \"darkblue\",\n              fill = \"black\") +\n  ggtitle(\"Weight and PCB Levels of Fish Without Outliers (2001-2011)\") +\n  labs(x = \"Weight (g)\",\n       y = \"Total PCB (ppm)\")\npcb_line\n\n\n\n\n\n\n\nCode\nSpeciesTable &lt;- table(pcb$Species.Name)\nspecies_counts_df &lt;- as.data.frame(SpeciesTable)\ncolnames(species_counts_df) &lt;- c(\"Species\", \"Count\")\n\nspecies_counts_df %&gt;%\n  kable(\"html\") %&gt;%\n  kable_classic(full_width = FALSE, html_font = \"Cambria\") %&gt;%\n  row_spec(c(1, 3, 5), background = \"#E6F7FF\") %&gt;%\n  row_spec(c(0, 2, 4), background = \"white\")\n\n\n\n\n\nSpecies\nCount\n\n\n\n\nBrown Bullhead\n262\n\n\nCatfish (channel)\n159\n\n\nCatfish (White)\n94\n\n\nStriped Bass\n1697\n\n\nYellow Perch\n260\n\n\n\n\n\n\n\n\nThe graph above plots a linear regression model of fish weight and how it corresponds to PCB levels found. The outliers were removed because it wouldn't be possible to see a trend and their inclusion made no difference to the trend line. The graph shows that there is barely a correlation between the weight of the fish and PCB content. The trend does increase slightly but not enough to see that there is a direct connection. This result is interesting because usually, larger fish tend to have larger concentrations of PCB. This is due to PCB having a tendency to accumulate in fat cells. There, however, is a reasonable explanation for this result. Most of the fish collected were under 5000 grams and as you can see in the species table above, the striped bass species heavily dominated the dataset.  What's particularly unique about striped bass is that they're migratory and only spend a small chunk of their life in the Hudson River.  Striped bass in their early stages, may reside in areas of the Hudson River highly affected by contamination and may exhibit elevated PCB levels compared to their older counterparts that may live in less polluted environments, such as the ocean or areas near New York City.\n\n\n\nSTL Decomposition\n\n\nCode\n# look at time series\npcb_ts &lt;- pcb %&gt;% \n  mutate(year_month = yearmonth(SDATE)) %&gt;% \n  group_by(year_month) %&gt;% \n  summarize(ts_pcb_mean = mean(Total.PCB.ppm., na.rm = TRUE)) %&gt;% \n  as_tsibble(index = year_month) %&gt;% \n  fill_gaps() %&gt;% \n   mutate(ts_pcb_mean = if_else(is.nan(ts_pcb_mean), NA, ts_pcb_mean) %&gt;% na.approx())\n\n# plot components\npcb_ts %&gt;% \n  model(STL(ts_pcb_mean)) %&gt;% \n  components() %&gt;% \n  autoplot(color = \"darkblue\") +\n  labs(title = \"STL Decomposition of PCB Content\",\n       x = \"Year Month\")\n\n\n\n\n\n\nThe figure above is a STL Decomposition of PCB content over the years and months. The main focus on this figure is the season_year row. Based on this, it is clear that there is a trend in how the season of the year affects the PCB Content found in the fish. During the Spring, The PCB content found seems to be at its lowest and then continues to rise until Fall. It then decreases again and continues this pattern for the entirety of the study. This seems to show that the temperature throughout the year plays a role in PCB content found in fish."
  },
  {
    "objectID": "blog/hudson_river_pcb_analysis/hudson_river_analysis.html#limitations",
    "href": "blog/hudson_river_pcb_analysis/hudson_river_analysis.html#limitations",
    "title": "Analyzing PCB Content In Hudson River Fish",
    "section": "Limitations",
    "text": "Limitations\nSome potential limitations that may have occurred when going through this study could have been human error when collecting data. Human error plays a big role in the project whether it be when the NYSDEC collects or processes the fish. The reason why there were so few larger fish in the dataset could have been just because larger fish are much harder to catch. When processing the fish, there is always room for human error whenever sample manipulation is done."
  },
  {
    "objectID": "blog/hudson_river_pcb_analysis/hudson_river_analysis.html#whats-next",
    "href": "blog/hudson_river_pcb_analysis/hudson_river_analysis.html#whats-next",
    "title": "Analyzing PCB Content In Hudson River Fish",
    "section": "What’s Next?",
    "text": "What’s Next?\nSo what's next for this study? Being able to understand how PCB levels change over time in the Hudson River can be very insightful and can be used in other future projects. The monitoring of PCB levels in the Hudson River is also still in effect and is relevant today. Since the dredging ended in 2015, PCB levels in the river have gone down significantly but aren't completely nonexistent. Making steps to remove the PCB entirely could be next in store for the Hudson River."
  },
  {
    "objectID": "blog/Thomas_fire_blog/Thomas_Fire_Blog.html",
    "href": "blog/Thomas_fire_blog/Thomas_Fire_Blog.html",
    "title": "Air Quality Data & False Color Image of the Thomas Fire",
    "section": "",
    "text": "Thomas Fire Aftermath"
  },
  {
    "objectID": "blog/Thomas_fire_blog/Thomas_Fire_Blog.html#about",
    "href": "blog/Thomas_fire_blog/Thomas_Fire_Blog.html#about",
    "title": "Air Quality Data & False Color Image of the Thomas Fire",
    "section": "About",
    "text": "About\nThe Thomas Fire, a destructive wildfire that unfolded in December 2017, struck Ventura County, California. It ignited near Santa Paula, the fire quickly gained momentum, fueled by powerful Santa Ana winds and dry environmental conditions. This formidable combination led to the rapid spread of the flames, ultimately encompassing over 281,000 acres, making it one of the largest wildfires in California’s history. Communities, including the city of Ventura, faced widespread evacuations as the fire approached populated areas, causing significant destruction with the loss of numerous homes and structures. Firefighters tirelessly battled the blaze amid challenging conditions, and it took several weeks to achieve full containment by January 12, 2018. The Thomas Fire prompted critical discussions about the increasing threat of wildfires in California and the necessity for enhanced firefighting and prevention strategies."
  },
  {
    "objectID": "blog/Thomas_fire_blog/Thomas_Fire_Blog.html#purpose",
    "href": "blog/Thomas_fire_blog/Thomas_Fire_Blog.html#purpose",
    "title": "Air Quality Data & False Color Image of the Thomas Fire",
    "section": "Purpose",
    "text": "Purpose\nThe purpose of this notebook is to create a graph of the Air Quality Data in Santa Barbara, CA over time. The other goal is to create a false color image showing the fire scar of the Thomas fire in 2017. The reason for these maps is so we can compare them to see when and how the Thomas fire affected Santa Barbara."
  },
  {
    "objectID": "blog/Thomas_fire_blog/Thomas_Fire_Blog.html#highlights",
    "href": "blog/Thomas_fire_blog/Thomas_Fire_Blog.html#highlights",
    "title": "Air Quality Data & False Color Image of the Thomas Fire",
    "section": "Highlights",
    "text": "Highlights\n\nData tidying using pandas and geopandas\nVisualizing data\nComparing data\nStatistical analysis"
  },
  {
    "objectID": "blog/Thomas_fire_blog/Thomas_Fire_Blog.html#data",
    "href": "blog/Thomas_fire_blog/Thomas_Fire_Blog.html#data",
    "title": "Air Quality Data & False Color Image of the Thomas Fire",
    "section": "Data",
    "text": "Data\n\nAir Quality Index (AQI) from the US Environmental Protection Agency website. This data will be used to visualize the impact on the AQI of the 2017 Thomas Fire in Santa Barbara County.\nA simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Surface Reflectance, collected by the Landsat 8 satellite. The data was accessed and pre-processed in the Microsoft Planetary Computer to remove data outside land and coarsen the spatial resolution (Landsat Collection in MPC). Data should be used for visualization purposes only.\nA shapefile of the fire perimeters in California during 2017. The complete file can be accessed in the CA state geoportal."
  },
  {
    "objectID": "blog/Thomas_fire_blog/Thomas_Fire_Blog.html#import-libraries-and-functions",
    "href": "blog/Thomas_fire_blog/Thomas_Fire_Blog.html#import-libraries-and-functions",
    "title": "Air Quality Data & False Color Image of the Thomas Fire",
    "section": "Import Libraries and Functions",
    "text": "Import Libraries and Functions\n\nimport os\nimport numpy as np\nimport pandas as pd\n\nimport geopandas as gpd\nimport xarray as xr\nimport rioxarray as rioxr\nimport matplotlib.patches as mpatches\nfrom matplotlib.markers import MarkerStyle\n\nfrom shapely import Point\nfrom shapely import Polygon\nfrom shapely import box\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\n\n\nImport and Read Data\n\n# read in the air data using link\naqi_17 = pd.read_csv(\"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip\")\naqi_18 = pd.read_csv(\"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip\")\n\n# read in fire dataset\n# create path\nca_bands = os.path.join(os.getcwd(),'Air-Quality-Data-False-Color-Image', 'data', 'landsat8-2018-01-26-sb-simplified.nc')\n\n# read in data using path\nca_fire_bands_2017 = rioxr.open_rasterio(ca_bands)\n\n# read in CA perimeter dataset\nca_fire_perimeter_2017= gpd.read_file(os.path.join(os.getcwd(),'Air-Quality-Data-False-Color-Image', 'data', 'California_Fire_Perimeters_2017', 'California_Fire_Perimeters_2017.shp'))"
  },
  {
    "objectID": "blog/Thomas_fire_blog/Thomas_Fire_Blog.html#data-analysis",
    "href": "blog/Thomas_fire_blog/Thomas_Fire_Blog.html#data-analysis",
    "title": "Air Quality Data & False Color Image of the Thomas Fire",
    "section": "Data Analysis",
    "text": "Data Analysis\n\nCreate A Rolling Mean\n\n# adds the mean of the AQI over a 5-day rolling window to the aqi_sb dataframe\naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean()"
  },
  {
    "objectID": "blog/Thomas_fire_blog/Thomas_Fire_Blog.html#aqi-plot",
    "href": "blog/Thomas_fire_blog/Thomas_Fire_Blog.html#aqi-plot",
    "title": "Air Quality Data & False Color Image of the Thomas Fire",
    "section": "AQI Plot",
    "text": "AQI Plot\n\naqi_sb.plot(y = ['aqi', 'five_day_average'], #plot aqi and five_day_average\n           title = 'AQI in Santa Barbara', #adds title\n           xlabel = 'Year', #adds x label\n           ylabel = 'Air Quality Index (AQI)', #adds y label\n           color = {'aqi': 'green', #adds color to aqi\n                    'five_day_average': 'pink' #add pink to five_day_average\n                   }\n           )\n\n# save plot into folder\nplt.savefig('Air-Quality-Data-False-Color-Image/figures/aqi.png', bbox_inches='tight',  dpi=50)\n\n\n\n\nAround December 2017, There was a large AQI spike in both daily and 5 day average. This was probably due to a fire at the beginning of that year."
  },
  {
    "objectID": "blog/Thomas_fire_blog/Thomas_Fire_Blog.html#map-of-false-color-image-and-thomas-fire-perimeter",
    "href": "blog/Thomas_fire_blog/Thomas_Fire_Blog.html#map-of-false-color-image-and-thomas-fire-perimeter",
    "title": "Air Quality Data & False Color Image of the Thomas Fire",
    "section": "Map of False Color Image and Thomas Fire Perimeter",
    "text": "Map of False Color Image and Thomas Fire Perimeter\n\n# creates empty plot\nfig, ax = plt.subplots(figsize = (7,7))\n\n\n# plot ca fire bands to use as base\nca_fire_bands_2017[['swir22', # subset for bands\n                     'nir08', \n                     'red']].to_array().plot.imshow(ax = ax,\n                                                    robust = True)\n\n# plot thomas fire perimeter\nthomas_perimeter.plot(ax = ax, edgecolor = \"purple\", color = 'none') # plot thomas_perimeter\nthomas_perimeter_patches = mpatches.Patch( color = \"purple\", # color\n                                    label = \"Thomas Fire Perimeter\") # label\n\n# title\nax.set_title('2017 Thomas Fire Perimeter',\nfontsize=22) # font size\n\n# add legend\nax.legend(handles = [thomas_perimeter_patches], frameon=False, loc='upper left', bbox_to_anchor = (1, 1))\n\n# save map into folder\nplt.savefig('Air-Quality-Data-False-Color-Image/figures/thomas_fire.png', bbox_inches='tight',  dpi=50)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Benjamin Versteeg",
    "section": "",
    "text": "My name is Ben Versteeg and I am in the Master of Environmental Data Science program at UC Santa Barbara."
  },
  {
    "objectID": "index.html#hi",
    "href": "index.html#hi",
    "title": "Benjamin Versteeg",
    "section": "",
    "text": "My name is Ben Versteeg and I am in the Master of Environmental Data Science program at UC Santa Barbara."
  },
  {
    "objectID": "index.html#job-experience",
    "href": "index.html#job-experience",
    "title": "Benjamin Versteeg",
    "section": "Job Experience",
    "text": "Job Experience\nExtra Help Employee - Orange County Environmental Resources Department"
  }
]